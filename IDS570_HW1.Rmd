---
title: "IDS_572_HW1"
author: "Nithin S Gowda,Akshay,lavisha"
date: "2022-09-28"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


# Problem 4 - Build Machine learning model for medical diagnoses

### Loading the necessary libraries
```{r}
library(rpart)  #partining of DT
library(caret) #To partition the data into test and training
library(dplyr)
library(rpart.plot)
library(data.tree)  
library(caTools) #Manipulation of data
library(ggplot2)
library(tidyr)
library(outliers)
library(sqldf)
library(dlookr)
library(corrplot)
library(aqp)
library(soilDB)
library('pROC') 
library(ROCR)
library("randomForest")
library(RColorBrewer)

```

### Importing all the datasets

```{r}
df_testX <- read.csv("testX.csv", header = FALSE)
df_testY <- read.csv("testY.csv", header = FALSE)
df_trainX <- read.csv("trainX.csv", header = FALSE)
df_trainY <- read.csv("trainY.csv", header = FALSE)
head(df_testX)
head(df_testY)
head(df_trainX)
head(df_trainY)
```

## Part a 
### Renaming all the columns

```{r}
names(df_testX) <- c('radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean','compactness_mean',
                     'concavity_mean','concave_points_mean','symmetry_mean','fractal_dimension_mean','radius_se',
                     'texure_se','perimeter_se','area_se','smoothness_se','compactness_se','concavity_se',
                     'concave_points_se','symmetry_se','fractal_dimension_se','radius_worst','texture_worst',
                     'perimeter_worst','area_worst','smoothness_worst','compactness_worst','concavity_worst',
                     'concave_points_worst','symmetry_worst','fractal_dimension_worst')

names(df_trainX) <- c('radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean','compactness_mean',
                     'concavity_mean','concave_points_mean','symmetry_mean','fractal_dimension_mean','radius_se',
                     'texure_se','perimeter_se','area_se','smoothness_se','compactness_se','concavity_se',
                     'concave_points_se','symmetry_se','fractal_dimension_se','radius_worst','texture_worst',
                     'perimeter_worst','area_worst','smoothness_worst','compactness_worst','concavity_worst',
                     'concave_points_worst','symmetry_worst','fractal_dimension_worst')


## Using `cbind()` function to join df_testX and df_textY and df_trainX and df_trainY

df_test <- cbind(df_testX,df_testY)
df_train <- cbind(df_trainX,df_trainY)
df_test
df_train


df_test <- df_test %>% rename(diagnosis = V1)
df_train <- df_train %>% rename(diagnosis = V1)

```


### Univariate analysis - Finding the summary statistics of each column
```{r}
#structure of dataframe
str(df_test)
str(df_train)

#Findings : There are 31 fields with 57 rows in train dataset
          # There are 31 fields with 455 rows in train dataset

#summary
summary(df_test)
summary(df_train)
```



### Data cleaning
```{r}
# Finding the missing values in the dataset using colSums

colSums(is.na(df_train))
#Findings : There are no missing records in the train dataset

# Finding outliers and treating them

# Plotting Boxplot to find the outliers
#boxplot(df_train,las=3.8,main = "Outlier detection of all columns using box plot")

## Selecting z-score over inter-quartile range because data has lot of outliers and if we use inter_quartile range then we would be removing around 236 records due to this dataset population would decrease dramatically.

z_scores <- as.data.frame(sapply(df_train, function(df_train) (abs(df_train-mean(df_train))/sd(df_train))))
Final_train_data <- df_train[!rowSums(z_scores>3), ]
dim(Final_train_data)
# Removed 56 outliers using Z-score method

boxplot(Final_train_data,las=3.8,main = "Box plot after outlier treatment")

```


### Bivariate analysis

### Correlation matrix

```{r}
# Calculate correlation matrix

df_correlationMatrix <- cor(Final_train_data)

# summarize the correlation matrix

print(df_correlationMatrix)

#Plot correlation matrix
corrplot(df_correlationMatrix, type = "upper",order = "hclust",col=brewer.pal(n=8,name= "RdYlBu"),tl.cex=0.5)
# From the output we can see that columns perimeter_worst, concave_points_mean, concave_poitns_worst,Texture_mean,area_mean,radius_worst,area_worst are highly correlated with diagnosis column

# find attributes that are highly corrected (ideally >0.75)

df_highlyCorrelated <- findCorrelation(df_correlationMatrix, cutoff=0.8,verbose = TRUE)
# print indexes of highly correlated attributes

print(df_highlyCorrelated)

# Plot Diagnosis vs Perimeter_worst
ggplot(Final_train_data, aes(x=perimeter_worst, fill=diagnosis)) + geom_histogram() + facet_wrap(~diagnosis)

# Plot Diagnosis vs concave_points_mean
ggplot(Final_train_data, aes(x=concave_points_mean, fill=diagnosis)) + geom_histogram() + facet_wrap(~diagnosis)

# Plot Diagnosis vs concave_points_worst
ggplot(Final_train_data, aes(x=concave_points_worst, fill=diagnosis)) + geom_histogram() + facet_wrap(~diagnosis)

# Plot Diagnosis vs texture_mean
ggplot(Final_train_data, aes(x=texture_mean, fill=diagnosis)) + geom_histogram() + facet_wrap(~diagnosis)

# Plot Diagnosis vs area_mean
ggplot(Final_train_data, aes(x=area_mean, fill=diagnosis)) + geom_histogram() + facet_wrap(~diagnosis)

# Plot Diagnosis vs radius_worst
ggplot(Final_train_data, aes(x=radius_worst, fill=diagnosis)) + geom_histogram() + facet_wrap(~diagnosis)

```


### Converting diagnosis column into factor

```{r}
Final_train_data$diagnosis <- as.factor(Final_train_data$diagnosis) # Converting the column to a factor variable

df_test$diagnosis <- as.factor(df_test$diagnosis) #Converting the column to a factor variable
```

### Part b - Create a Decision tree using information gain splits

```{r}
DT <- rpart(diagnosis ~ ., data=Final_train_data,parms = list(split="information") ,method="class")
summary(DT)
# Plotting decision tree using rpart.plot()
rpart.plot(DT, main="Decision Tree for medical diagnoses")
 plotcp(DT)

 # Feature evaluation of decision tree
 DT_feature <- data.frame(imp = DT$variable.importance)
DT_feature1 <- DT_feature %>% 
  tibble::rownames_to_column() %>% 
  dplyr::rename("variable" = rowname) %>% 
  dplyr::arrange(imp) %>%
  dplyr::mutate(variable = forcats::fct_inorder(variable))

ggplot2::ggplot(DT_feature1) +
  geom_col(aes(x = variable, y = imp),
           col = "black", show.legend = F) +
  coord_flip() +
  scale_fill_grey() +
  theme_bw()
# The important features predicted from decision tree are perimeter_worst, radius_worst, area_worst,perimeter_mean,area_mean,perimter_mean,area_mean,radius_mean,cancave_points_worst,concavity_mean
 
 # Insights : There are 5 leaf nodes in this decision tree
```




### Part c - Major predictors

--Major predictors suggested by the tree are Perimeter_worst, concave_points_mean, Conace_poins_worst,Texure_mean. These are the major predictors because we are getting maximum information gain from each split

--Yes,Predictors from the Decision tree is same as the predictors we got from correlation matrix

### Part d - Highest probabilty of cancer

-- If the perimeter_worst is less than 107 and concave_points_mean greater than 0.064, then the person is likely to have cancerous tissue and the probability in this case is 100% -- If the perimeter perimeter_worst is less than 107 and concave_points_mean is less than 0.064 and texture_worst is less than 20 then the person having cancerous tissue has a probability of 81%

### Part e - Accuracy of the decision tree model

```{r}
# Predicting the model on train data
predict_train <-predict(DT, Final_train_data, type = 'class')
table_train <- table(Final_train_data$diagnosis, predict_train)
table_train

# Predicting the model on test data
predict_test <-predict(DT, df_test, type = 'class')
table_test <- table(df_test$diagnosis, predict_test)
table_test

# Accuracy of the model on train data
accuracy_Train <- sum(diag(table_train)) / sum(table_train)
print(paste('Accuracy for train', accuracy_Train))
# Findings : Accuracy for the train data is 96.2%

# Accuracy of the model on test data
accuracy_Test <- sum(diag(table_test)) / sum(table_test)
print(paste('Accuracy for test', accuracy_Test))
#Accuracy for test data is 89.4%

```


### Part f - Constructing the best possible decision tree

```{r}
# Bulding a new decision tree to improve the accuracy

DT1 <- rpart(diagnosis ~ ., data=Final_train_data,parms = list(split="information") ,method="class",
             control = rpart.control( minsplit = 10, minbucket = 5, cp = 0.01))

# Summary of decision tree
summary(DT1)

# Predicting the model on train data
DT1_train <- table(pred=predict(DT1,Final_train_data, type="class"), true=Final_train_data$diagnosis)


# Predicting the model on test data
DT1_test <- table(pred=predict(DT1,df_test, type="class"), true=df_test$diagnosis)


# Accuracy of train data

accuracy_Train_DT1 <- sum(diag(DT1_train)) / sum(DT1_train)
accuracy_Train_DT1
# Insights : Accuracy of train data is 98.74%

# Accuracy of test data

accuracy_Test_DT1 <- sum(diag(DT1_test)) / sum(DT1_test)
accuracy_Test_DT1
# Insights : Accuracy of test data is 94.73%

# Confusion matrix
confusionMatrix(DT1_train,reference = Final_train_data$diagnosis)

#Insights : 1. Accuracy : 98.75%
#           2.Sensitivity : 0.984       
#           3.Specificity : 0.984


# ROC curve for new model DT1
scoreTst <- predict(DT1, df_test, type="prob")[, 2]
scoreTst

#now apply the prediction function from ROCR to get a prediction object
rocPredTst <- prediction(scoreTst, df_test$diagnosis, label.ordering = c('0', '1')) 

#obtain performance using the function from ROCR, then plot
perfROCTst<-ROCR::performance(rocPredTst,"tpr","fpr")

# ROC curve for a initial model DT
scoreTst_DT <- predict(DT, df_test, type="prob")[,2]
rocPredTst_DT <- prediction(scoreTst_DT, df_test$diagnosis,label.ordering = c('0', '1'))
perfROCTst_DT <- ROCR::performance(rocPredTst_DT, "tpr", "fpr")
plot(perfROCTst)
plot(perfROCTst_DT, add=TRUE, col="blue")

# AUC value for Final model DT1
aucPerf_final=ROCR::performance(rocPredTst, "auc")
aucPerf_final@y.values

#Findings : AUC value for DT1 is 0.944

# AUC value for initial model DT
aucPerf_initial=ROCR::performance(rocPredTst_DT, "auc")
aucPerf_initial@y.values

# Findings : AUC value for DT is 0.9268

```

### Part g - Plot your final decision tree model and write down all decision rules

```{r}

rpart.plot(DT1, main="Final Decision Tree for medical diagnoses")

#Decision rules : 
#1. Diagnosis is the factor variable
#2. We are splitting it on information 
#3. We are using a minimum split of 10
#4. We are using minimum bucket of 5
#5. We are building a decision tree with complexity parameter(cp) 0.01
```

################################################################################
# Problem 5 - Build Machine learning model to detect phishing attacks
################################################################################
### Importing the Data

```{r}
df_phi <-read.csv("Training Dataset.arff", header = FALSE, comment.char = "@")

```


### Renaming the column names

```{r}

names(df_phi) <- c('having_IP_Address', 'URL_Length', 'Shortining_Service', 'having_At_Symbol', 'double_slash_redirecting', 'Prefix_Suffix', 'having_Sub_Domain', 'SSLfinal_State', 'Domain_registeration_length', 'Favicon', 'port', 'HTTPS_token', 'Request_URL', 'URL_of_Anchor', 'Links_in_tags', 'SFH', 'Submitting_to_email', 'Abnormal_URL', 'Redirect', 'on_mouseover', 'RightClick', 'popUpWidnow', 'Iframe', 'age_of_domain', 'DNSRecord', 'web_traffic', 'Page_Rank', 'Google_Index', 'Links_pointing_to_page', 'Statistical_report', 'Result')


# Assiging phishing and legitimate values in Result column
df_phi$Result <-replace(df_phi$Result , df_phi$Result == -1, 'phishing')
df_phi$Result <-replace(df_phi$Result , df_phi$Result == -1, 'legitimate')

head(df_phi)
```

#If we have only -1 and 1 in a column then -1 = phishing and 1 = legitimate #If we have -1,0,1 values in a column then -1=legitimate, 0=suspicious, 1= phishing

### Exploratory data analysis

```{r}
#structure of dataframe

str(df_phi)
#Findings : There are 31 fields with 11055 rows in train dataset

#summary
summary(df_phi) 

# Checking for missing values

colSums(is.na(df_phi))
# There are no missing values in the dataset

# Detecting outliers in the dataset
# Plotting Boxplot to find the outliers

boxplot(df_phi,main = "Outlier detection of all columns using box plot")
# Insights : As the data is either 0,1,-1 there are no outliers due to this we are not performing outlier treatment

# calculate correlation matrix

correlationMatrix <- cor(df_phi)

# summarize the correlation matrix

print(correlationMatrix)

#Plot correlation matrix

corrplot(correlationMatrix, type = "upper",order = "hclust",col=brewer.pal(n=8,name= "RdYlBu"),tl.cex=0.5)

# find attributes that are highly corrected (ideally >0.75)

highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.8,verbose = TRUE)

# print indexes of highly correlated attributes

print(highlyCorrelated)
# From the output we can see that columns SSLfinal_State, URL_of_Anchor, web_traffic, having_Sub_Domain, Domain_registeration_length,Request_URL are highly correlated with Result variable

# Plotting the relationships between important features and target variable

# Plot Result vs SSLfinal_State
ggplot(df_phi, aes(x=SSLfinal_State, fill=Result)) + geom_bar(position="dodge")

# Plot Result vs URL_of_Anchor
ggplot(df_phi, aes(x=URL_of_Anchor, fill=Result)) + geom_bar(position="dodge")

# Plot Result vs web_traffic
ggplot(df_phi, aes(x=web_traffic, fill=Result)) + geom_bar(position="dodge")

# Plot Result vs having_Sub_Domain
ggplot(df_phi, aes(x=having_Sub_Domain, fill=Result)) + geom_bar(position="dodge")

# Plot Result vs Domain_registeration_length
ggplot(df_phi, aes(x=Domain_registeration_length, fill=Result)) + geom_bar(position="dodge")

# Plot Result vs Request_URL
ggplot(df_phi, aes(x=Request_URL, fill=Result)) + geom_bar(position="dodge")
```

## Converting all fields to factor variable

```{r}
df_phi <- mutate_if(df_phi, is.numeric, as.factor)
```


## Developing and evaluating the Decision tree model using cross validation techniques
```{r}
# Splitting the dataset into train and test using random sample
set.seed(1234)
sample <- sample(c(TRUE, FALSE), nrow(df_phi), replace=TRUE, prob=c(0.8,0.2))
phi_train  <- df_phi[sample, ]
phi_test   <- df_phi[!sample, ]
head(phi_train)
head(phi_test)

# constructing Decision tree 
DT_phi <- rpart(Result ~ ., data=phi_train,parms = list(split="information") ,method="class")
summary(DT_phi)

# Plotting decision tree using rpart.plot()
rpart.plot(DT_phi, main="Decision Tree for Website phishing")

# Feature evaluation of decision tree
 phi_feature <- data.frame(imp = DT_phi$variable.importance)
phi_feature1 <- phi_feature %>% 
  tibble::rownames_to_column() %>% 
  dplyr::rename("variable" = rowname) %>% 
  dplyr::arrange(imp) %>%
  dplyr::mutate(variable = forcats::fct_inorder(variable))

ggplot2::ggplot(phi_feature1) +
  geom_col(aes(x = variable, y = imp),
           col = "black", show.legend = F) +
  coord_flip() +
  scale_fill_grey() +
  theme_bw()
# The important features predicted from decision tree are SSLfinal_State, URL_of_Anchor, web_traffic, having_Sub_Domain, Domain_registeration_length,Request_URL


######################
# Predicting the model on train data

phi_predict_train <-predict(DT_phi, phi_train, type = 'class')
phi_table_train <- table(phi_train$Result, phi_predict_train)
phi_table_train

# Predicting the model on test data

phi_predict_test <-predict(DT_phi, phi_test, type = 'class')
phi_table_test <- table(phi_test$Result, phi_predict_test)
phi_table_test


##################
# Confusion matrix to calculate the performance of the decision tree

#Confusion Matrix for train data
confusionMatrix(phi_table_train,reference = phi_train$Result)

#Findings :
# 1. Accuracy of the decision tree on training data is 90.3%
# 2. Sensitivity of the decision tree on training data is 0.88
# 3. Specificity of the decision tree on training data is 0.92

#Confusion Matrix for test data
confusionMatrix(phi_table_test,reference = phi_test$Result)

#Findings :
# 1. Accuracy of the decision tree on test data is 91.56%
# 2. Sensitivity of the decision tree on test data is 0.90
# 3. Specificity of the decision tree on test data is 0.92

###################

# Cross validation of decision tree on test data

Validation_DT_model <- data.frame( R2 = R2(as.numeric(phi_predict_test), as.numeric(phi_test$Result)),
            RMSE = RMSE(as.numeric(phi_predict_test), as.numeric(phi_test$Result)),
            MAE = MAE(as.numeric(phi_predict_test), as.numeric(phi_test$Result)))
Validation_DT_model

# Findings :
# 1. R2 Error is 0.688
# 2. Mean Absolute Error (MAE) is 0.29
# 3. Root Mean Squared Error (RMSE) is 0.084


```

## Random forest model and evaluation
```{r}
# Set a random seed
set.seed(51)

# Training the model using random forest model

rf_model <- randomForest(formula = Result ~ .,data= phi_train,ntree=1000,nodesize = 10)
rf_model

# Predicting the Test set results
rf_pred = predict(rf_model, newdata = phi_test)
rf_table_test <- table(phi_test$Result, rf_pred)

  
# Confusion Matrix
confusionMatrix(rf_table_test,reference = phi_test$Result)

#Findings :
# 1. Accuracy of the random forest model on test data is 96.63%
# 2. Sensitivity of the random forest model on test data is 0.967
# 3. Specificity of the random forest model on test data is 0.956

plot(rf_model)
  
# Importance plot
importance(rf_model)
  
# Variable importance plot
varImpPlot(rf_model)

# Cross validation 

Validation_rf_model <- data.frame( R2 = R2(as.numeric(rf_pred), as.numeric(phi_test$Result)),
            RMSE = RMSE(as.numeric(rf_pred), as.numeric(phi_test$Result)),
            MAE = MAE(as.numeric(rf_pred), as.numeric(phi_test$Result)))
Validation_rf_model

# Findings :
# 1. R2 Error is 0.88 high
# 2. Mean Absolute Error (MAE) is 0.17 Low
# 3. Root Mean Squared Error (RMSE) is 0.030 Low
```
#### We would be selecting the random forest model over decision tree because of following reasons                     1. Random forest has higher R2 (0.86) than decision tree (0.688).Better model should have a high value of R-squared so Random forest is the better model                                                                                      2. Mean Absolute Error (MAE) of random forest is lower(0.183) than decision tree(0.29). Better model should have low value of MAE so choosing random forest model                                                                           3. Root Mean Squared Error (RMSE) of random forest is lower than decsion tree so it the better model                   4. Accuracy of random forest model is 5% more than decision tree model so we conclude that random forest is the better model






